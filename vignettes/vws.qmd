---
title: "`vws`: Vertical Weighted Strips in R using C++"
author: Andrew M. Raim, James A. Livsey, and Kyle M. Irimata
format:
  pdf:
    mathspec: true
    fontsize: 10pt
    indent: false
    toc: true
    number-sections: true
    colorlinks: true
    link-citations: true
    prompt: false
    include-in-header:
      text: |
        \usepackage{common}
pdf-engine: pdflatex
vignette: >
  %\VignetteIndexEntry{vws}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{quarto::pdf}
bibliography: references.bib
editor_options: 
  chunk_output_type: console
abstract: |
  TBD abstract goes here.
thanks: |
  Center for Statistical Research & Methodology, U.S. Census Bureau,
  Washington, DC, 20233, U.S.A.
  **For correspondence**`:` <andrew.raim@gmail.com>.
  **Disclaimer**`:` This document is released to inform interested parties
  of ongoing research and to encourage discussion of work in progress. Any
  views expressed are those of the authors and not those of the U.S. Census
  Bureau.
  Document was compiled `{r} format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")`.
geometry:
  - left=0.5in
  - right=0.5in
  - top=0.75in
  - bottom=1.00in
execute:
  eval: false
code-block-bg: true
code-block-border-left: "#B2DEED"
filters:
  - include-code-files
callout-icon: false
---

```{r}
#| include: false
library(vws)
library(tidyverse)

set.seed(1234)
```

# TBD {-}
::: {.callout-caution}
- Consider suppressing plotting code from the document if it gets too lengthy.
  We can mention to readers that the plotting code can be found with the
  vignette sources, or try to get them installed with the package perhaps.

- We should have a separate section where we describe doing stuff: constructing
  the proposal (special cases and general case where we override abstract
  region), refining the proposal, ...
:::

# Introduction

::: {.callout-caution title="TBD"}
This section is heavily under construction.

- Package is available from <https://github.com/andrewraim/vws> and possibly on
  CRAN(?).
- Some of the lengthier codes are not given in this document, but are
  provided in external files. These can be accessed by the reader from the
  installed `vws` package in the folder `doc/examples`. They can find this with
  the command `file.path(path.package("vws"), "doc", "examples")`.
- An `R>` prompt is shown in some code displays to emphasize interaction via
  the console.
- For the first example, we should explain things in a lot of detail. After
  that, we can give less detail, except any new aspects.
- Let $\ind\{ x \in A \}$ be the indicator function for the event $[x \in A]$.
- We will use `ggplot2`, `dplyr` and other packages of the tidyverse
  [@Tidyverse2019] in the examples.
- Remark: We don't use Rcpp at the moment because interoperability between C++
  classes and R becomes more complicated. Therefore, the programming model is to
  code samplers in C++ and expose them as an R function
- Remark: Make sure to mention that other region types (e.g., multivariate) can be
  implemented with the API.

:::

To implement a rejection sampler with the `vws` package, the user must make use
of an existing subclass of `Region` or implement a new one. Two such subclasses
included in the package implement the univariate "constant VWS" method described
by @VWS2025:

- `RealConstRegion`: constant VWS with a continuous support.
- `IntConstRegion`: constant VWS with an integer support.

See the manual pages for each sampler for usage and examples. These two regions
are somewhat flexible and can be refined to a variety of problems. In this
vignette, we will describe the process of coding a customized region. We will
give one univariate example that makes use of the linear VWS method for
improved efficiency, and one multivariate example where the regions go beyond
intervals.

**TBD**: The example sections illustrate VWS samplers implemented using the `vws` package. The first example in [@sec-ln-norm] is described in the most explicit level of detail.


# A Brief Review of Vertical Weighted Strips {#sec-vws}

The objective of VWS is to sample from a weighted density
<!-- -->
\begin{align}
f(x) = f_0(x) / \psi, \quad
f_0(x) = w(x) g(x), \quad
\psi = \int_\Omega f_0(x) d\nu(x),
\label{eqn:weighted-target}
\end{align}
<!-- -->
where $\Omega$ is the support, $\nu$ is a dominating measure, $g$ is assumed to be a normalized density, $w(x)$ is a nonnegative weight function, and $\psi$ is a normalizing constant. We will construct a proposal of the form
<!-- -->
\begin{align*}
h(x) = h_0(x) / \psi_N, \quad
h_0(x) = \overline{w}(x) g(x), \quad
\psi_N = \int_\Omega h_0(x) d\nu(x).
\end{align*}
<!-- -->
The construction assumes that $\Omega$ is partitioned into regions $\mathcal{D}_1, \ldots, \mathcal{D}_N$ and there are corresponding functions $\overline{w}_j$ such that $\overline{w}_j(x) \geq w(x)$ for each $x \in \mathcal{D}_j$. We say that $\overline{w}_j$ *majorizes* $w$ on $\Omega$. Taking $\overline{w}$ as $\overline{w}(x) = \sum_{j=1}^N \overline{w}_j(x) \ind\{x \in \mathcal{D}_j\}$, the unnormalized proposal becomes
<!-- -->
\begin{align*}
h_0(x) = g(x) \sum_{j=1}^N \overline{w}_j(x) \ind\{x \in \mathcal{D}_j\}.
\end{align*}
<!-- -->
With this construction, $f_0(x) \leq h_0(x)$ for all $x \in \Omega$. Therefore, classical rejection sampling can be carried out by drawing $u$ from $\text{Uniform}(0,1)$, $x$ from $h$, and accepting $x$ as a draw from $f$ if $u \leq f_0(x) / h_0(x)$. The normalized $h$ can be obtained by defining $\overline{\xi}_j = \E[\overline{w}_j(T) \ind\{T \in \mathcal{D}_j\}]$ with $T \sim g$ and $\psi_N = \sum_{j=1}^N \overline{\xi}_j$, giving the finite mixture
<!-- -->
\begin{align}
h(x)
= h_0(x) / \psi_N
= \sum_{j=1}^N \pi_j g_j(x),
\label{eqn:fmm-proposal}
\end{align}
<!-- -->
Equation \eqref{eqn:fmm-proposal} is seen to be a finite mixture with mixing weights $\pi_j = \overline{\xi}_j / \{ \sum_{\ell=1}^N \overline{\xi}_\ell \}$, and component densities
<!-- -->
\begin{align*}
g_j(x) = 
\overline{w}_j(x) g(x) \ind\{x \in \mathcal{D}_j\} / \overline{\xi}_j,
\end{align*}
<!-- -->
which are truncated and reweighted versions of base distribution $g$. In addition to the majorizer, suppose that $\underline{w}_j$ is a minorizer of $w$ so that
$0 \leq \underline{w}_j(x) \leq w(x)$ for all $x \in \mathcal{D}_j$,
and let $\underline{\xi}_j = \E[\underline{w}_j(T) \ind\{T \in \mathcal{D}_j\}]$ with $T \sim g$. When $h$ is used as a proposal in rejection sampling, an upper bound for the probability of rejection is
<!-- -->
\begin{align}
1 - \frac{\sum_{j=1}^N \underline{\xi}_j}{\sum_{j=1}^N \overline{\xi}_j}.
\label{eqn:bound}
\end{align}
<!-- -->
This bound can be used to quickly determine whether the proposal will be viable for rejection sampling. If the bound is seen to be large, the proposal may be refined by altering the partition or considering a different majorizer. Several specific choices of majorizer are considered by @VWS2025 and will be reviewed in the present document. [@sec-vws-constant] discusses the use of a constant function. A linear function is discussed in @sec-vws-linear.

## Constant Majorizer {#sec-vws-constant}

**TBD: Make this only about the method. Code will be elsewhere**

`RealConstRegion` is a subclass of `Region` for a particular setting where operations can be coded in a relatively problem-agnostic way. Suppose $\Omega = (a,b]$ is an interval whose endpoints may or may not be finite. Furthermore, suppose decomposition \eqref{eqn:weighted-target} is selected so that $w(x)$ is finite on each $\mathcal{D}_j$ and the constant $\overline{w}_j = \sup_{x \in \mathcal{D}_j} w(x)$ can serve as the majorizing function of $w$. Furthermore, let the minorizer for $w$ be the constant $\underline{w}_j = \inf_{x \in \mathcal{D}_j} w(x)$. Here we obtain component densities
$g_j(x) = g(x) \ind\{x \in \mathcal{D}_j\} / \Prob(T \in \mathcal{D}_j)$
along with the quantities
$\overline{\xi}_j = \overline{w}_j \Prob(T \in \mathcal{D}_j)$ and
$\underline{\xi}_j = \underline{w}_j \Prob(T \in \mathcal{D}_j)$.

Several components are needed to construct a object of class `RealConstRegion`: scalars `a` and `b` define the support, a weight function `w`, and an object that provides the necessary operations for base distribution $g$. 

```{r}
#| eval: false
#| prompt: true
region = RealConstRegion$new(a, b, w, g)
```

The argument `w` is a standard R function, but is expected to have two arguments: the first argument is the input to the function and the second argument `log` indicates whether the result should be returned on the log-scale (`log = TRUE`) or the original scale (`log = FALSE`).

```{r}
#| eval: false
#| prompt: true
w = function(x, log = TRUE) { ... }
```

For the argument `g`, the `vws` package provides a `univariate_helper` function that wraps these operations together in an `S3` object. Here is an example with $g$ as the $\text{N}(\mu, \sigma^2)$ distribution.

```{r}
mu = 0
sigma = 1

g = univariate_helper(
	d = function(x, log = FALSE) {
		# Density function with mean and sd fixed to mu and sigma
		dnorm(x, mean = mu, sd = sigma, log = log)
	},
	p = function(q, lower.tail = TRUE, log.p = FALSE) {
		# CDF function with mean and sd fixed to mu and sigma
		pnorm(q, mean = mu, sd = sigma, lower.tail = lower.tail, log.p = log.p)
	},
	q = function(p, lower.tail = TRUE, log.p = FALSE) {
		# Quantile function with mean and sd fixed to mu and sigma
		qnorm(p, mean = mu, sd = sigma, lower.tail = lower.tail, log.p = log.p)
	},
	s = function(x) {
		# Indicator of whether x is in the support of this distribution.
		is.numeric(x)
	}
)
```

Because the $\text{N}(\mu, \sigma^2)$ distribution is used as a
`univariate_helper` in `vws` package demonstrations, the following function is
provided as a shortcut to return the same structure as the call above.

```{r}
g = normal_helper(mu, sigma)
```

For a similar setting with an integer-valued support, the subclass
`IntConstRegion` of `RealConstRegion` may be considered. Usage
of `IntConstRegion` is similar to `RealConstRegion`, with some
implementation details customized to the integer case. In this setting, `g`
should be a `univariate_helper` based on an integer-valued distribution.

```{r}
#| eval: false
#| prompt: true
g = poisson_helper(lambda = 10)  ## A predefined helper for Poisson(lambda)
region = IntConstRegion$new(a, b, w, g)
```

A natural choice to partition a univariate $\Omega = (a,b]$ is to break it into intervals defined by knots $\alpha_0 < \cdots < \alpha_N$, with $\alpha_0 \equiv a$ and $\alpha_N \equiv b$ fixed. For a univariate support and a constant majorizer, the baseline strategy of `bifurcate` in the `RealConstRegion` class is to replace $\mathcal{D}_\ell = (\alpha_{\ell-1}, \alpha_{\ell}]$ with $\mathcal{D}_\ell^{(1)} = (\alpha_{\ell-1}, \alpha_{\ell^*}]$ and $\mathcal{D}_\ell^{(2)} = (\alpha_{\ell^*}, \alpha_\ell]$, where
<!-- -->
\begin{align*}
\alpha_{\ell^*} =
\begin{cases}
0 & \text{if $\alpha_{\ell-1} = -\infty$ and $\alpha_\ell = \infty$}, \\
\alpha_\ell - |\alpha_\ell| - 1 & \text{if $\alpha_{\ell-1} = -\infty$ and $\alpha_\ell < \infty$}, \\
\alpha_{\ell-1} + |\alpha_{\ell-1}| + 1 & \text{if $\alpha_{\ell-1} > -\infty$ and $\alpha_\ell = \infty$}, \\
(\alpha_{\ell-1} + \alpha_\ell) / 2 & \text{otherwise}.
\end{cases}
\end{align*}
<!-- -->
Here, `is_bifurcatable` is always taken to be `TRUE`. When the support is integer-valued, `RealConstRegion` instead considers
<!-- -->
\begin{align*}
\alpha_{\ell^*} =
\begin{cases}
0 & \text{if $\alpha_{\ell-1} = -\infty$ and $\alpha_\ell = \infty$}, \\
\alpha_\ell - |\alpha_\ell| - 1 & \text{if $\alpha_{\ell-1} = -\infty$ and $\alpha_\ell < \infty$}, \\
\alpha_{\ell-1} + |\alpha_{\ell-1}| + 1 & \text{if $\alpha_{\ell-1} > -\infty$ and $\alpha_\ell = \infty$}, \\
\lceil (\alpha_{\ell-1} + \alpha_\ell) / 2 \rceil & \text{otherwise}.
\end{cases}
\end{align*}
<!-- -->
The function `is_bifurcatable` returns `FALSE` for regions which contain no integers.

The `IntConstRegion` and `RealConstRegion` classes have an `optimize` method to compute the constants $\overline{w}_j$ and $\underline{w}_j$ numerically. 

```{r}
#| eval: false
#| prompt: true
region$optimize(maximize = TRUE, log = TRUE)
```

The arguments `maximize` and `log` are both indicators: optimization is carried out as a maximization if `maximize = TRUE` and a minimization otherwise; the optimized value is returned on the log-scale if `log = TRUE` and is returned on the original scale otherwise. Numerical optimization is convenient, but can be wasteful if the function $w$ can be maximized and/or minimized in closed-form. For such cases, the user may create a subclass of `IntConstRegion` or `RealConstRegion` and override the `optimize` method. An example of this is given in [@sec-ln-norm-const-custom].

## Linear Majorizer {#sec-vws-linear}

**TBD**: write about linear majorizer

## Knot Selection {#sec-vws-knots}

**TBD**

- Our rule of thumb (and greedy option)
- At specified knots


The `refine` function refines a given proposal, which is based on partition $\mathcal{D}_1, \ldots, \mathcal{D}_{N}$, by sequentially selecting and bifurcating regions $N'$ times to obtain a new partition $\mathcal{D}_1^*, \ldots, \mathcal{D}_{N + N'}^*$. The following rule of thumb seeks to reduce \eqref{eqn:bound}. Let
<!-- -->
\begin{align}
\rho_1 = \frac{
\overline{\xi}_1 - \underline{\xi}_1
}{
\sum_{j=1}^N \overline{\xi}_j
},
\quad \ldots, \quad
\rho_N = \frac{
\overline{\xi}_N - \underline{\xi}_N
}{
\sum_{j=1}^N \overline{\xi}_j
}
\label{eqn:bound-components}
\end{align}
<!-- -->
be the contribution of each region to \eqref{eqn:bound} when there are $N$ regions. We draw index $\ell$ from $(1, \ldots, N)$ with probabilities proportional to $\rho_1, \ldots, \rho_N$, then bifurcate region $\ell$ into regions $\mathcal{D}_\ell^{(1)}$ and $\mathcal{D}_\ell^{(2)}$. There may be a number of ways to define bifurcation when $\Omega$ is a multivariate set. For a given subclass of `Region`, the bifurcation approach is to be implemented in the `bifurcate` method. Additionally, each subclass of `Region` should implement the `is_bifurcatable` method which returns `FALSE` if a `Region` object should not be further bifurcated; otherwise it returns `TRUE`.

In the following example, a proposal `h_init` is refined `N = 10` times
to yield an improved proposal `h`.

```{R}
#| eval: false
#| prompt: true
h_init = FMMProposal$new(regions)
out = refine(h_init, N = 10)
h = out$h
```

In addition to the element `h` in the return value which represents the refined
proposal, the element `log_bdd_hist` contains a vector with values
<!-- -->
\begin{align*}
\log\left\{
1 - \frac{\sum_{j=1}^{N+t} \underline{\xi}_j^{(t)}}{\sum_{j=1}^{N+t} \overline{\xi}_j^{(t)}}
\right\}
\end{align*}
<!-- -->
computed at steps $t = 0, 1, \ldots, N'$ of the refinement process. Here, $\overline{\xi}_j^{(t)}$ and $\underline{\xi}_j^{(t)}$ respectively represent $\overline{\xi}_j$ and $\underline{\xi}_j$ at step $t$. Effectiveness of a call to `refine` can be evaluated by examining `log_bdd_hist`.

```{r}
#| eval: false
#| prompt: true
step = seq_along(out$log_bdd_hist) - 1  ## Make sequence 0, 1, ..., N
bdd = exp(out$log_bdd_hist)             ## Exponentiate to probability scale
plot(step, bdd, type = "l")
```

If the bound can be reduced to a sufficiently small probability, the user can be assured that the proportion of rejections will be relatively small during sampling. Note that the same call to `refine` may result in different `h` and `log_bdd_hist` outputs due to randomness in the refinement method.

# Preliminaries {#sec-prelim}

## Programming Model

**TBD**: Write samplers in C++ and expose them in R via Rcpp. Show a minimal
example of such a function here. Maybe mention that the first working example
is coming up in @sec-vmf or another example.

**TBD:** Emphasize that API is in the `vws` namespace.

## Lambdas in C++

**TBD**: Recall lambdas in general

**TBD:** Mention density, cdf, quantile function definitions and numerical tools via fntl.

## Log-Scale Arithmetic

**TBD:** Explain that computations are kept on log-scale in the package and why we do it.

## Custom Optimization {#sec-user-opt}

**TBD**

- Mention that we use a numerical method by default
- R version
- C++ version

# Overview of Package {#sec-overview}

The `vws` package aims to support the methodology which was described in the previous section. The present section will describe tools in the package which can be used to formulate a problem, construct a proposal, and generate samples.

::: {.callout-caution title="TBD"}
Misc

- How about the name `d_g` instead of `d_base`?
- We'll want to show some examples where we override `optimize` and maybe the
  bifurcate functions. Consider pointing to them somewhere in this section.
- Readers might want to check out one of the first / easiest examples (name
  one) to see the basic usage pattern, then jump back up here to see the
  components described in more detail. May want to say this before we start
  going into any detail.
:::

Use of the package focuses on two `R6` classes. The `FMMProposal` class represents finite mixture \eqref{eqn:fmm-proposal} and encapsulates operations needed for rejection sampling. The `Region` class represents region $\mathcal{D}_j$ and the operations that must be supported on it for VWS; i.e., all problem-specific logic is coded within a `Region`. An `FMMProposal` object is created from a list of one or more `Region` objects that represent the partition $\mathcal{D}_1, \ldots, \mathcal{D}_N$ of $\Omega$.

The `rejection` function takes an object `h` of class `FMMProposal` and carries out
the rejection sampling algorithm to obtain `n` draws. The return value of `rejection` is a list where each element represents an accepted draw. Optional arguments may be passed
through a `rejection_control` (S3) object, including: a count of rejections to be tolerated before halting, and whether to return additional information about rejections which occurred during sampling. The following display gives a typical workflow for sampling.

```{R}
#| eval: false
#| prompt: true
regions = list(region1, region2)
h = FMMProposal$new(regions)
ctrl = rejection_control(max_rejects = 5000)
rejection(h, n = 1000, control = ctrl)
```

```{r}
#| include: false
# If the interface has changed, catch it and bail out
actual = names(formals(rejection_control))
expected = c("max_rejects", "report", "extra_outputs", "action_incomplete")
base::setequal(actual, expected) |> stopifnot()
```

Here, `region1` and `region2` are objects whose class is a subclass of `Region`. The `Region` class itself is abstract and defines necessary operations. Further details are given in Sections [-@sec-cpp-api-region], [-@sec-cpp-api-realconstregion], and [-@sec-cpp-api-intconstregion].

Before sampling, the `refine` function can be used to refine a given
`FMMProposal` object by partitioning a given set of regions into a finer set.
The can make it a better approximation of the target distribution. Details are
given in Section [-@sec-cpp-api-proposal].

@fig-software-design displays a diagram of the high-level design just described. @sec-cpp-api walks through the API components in depth. The user may also consult manual entries (e.g., `?Region`) for details such as arguments to methods and their default values.

![Design of `vws` package.](software-design.pdf){#fig-software-design}

## Basic Usage {#sec-overview-usage}

**TBD:** Basic usage and functions here. Maybe not a working example here, but
refer the reader to one of the complete working examples that comes up later.
Also give a reference to the API section.

## Implementing Samplers via Regions {#sec-overview-regions}

**TBD:** Discuss regions here at a high level. This is the most important part of the API because it is how users implement problems. They can use `RealConstRegion` or `IntConstRegion` for univariate problems with constant majorizers. They can customize the optimization in these from the default numerical method. They can also subclass the abstract `Region` class to implement other support types.

For targets where the support is not univariate, or where the desired majorizer is something other than a constant, the user may create a subclass `Region` to implement VWS. This new class must implement the methods in Table `?` using `R6`. Here is a skeleton of a such a subclass named `CustomRegion` to illustrate this process. Complete implementations of such subclasses are given in Sections [-@sec-ln-norm-linear] and [-@sec-vws-linear].

## Knot Selection {#sec-overview-knots}

**TBD**

- Our rule of thumb (and greedy option)
- At specified knots


# C++ API {#sec-cpp-api}

This section documents functions, classes, and other components in the C++ API. 

## Typedefs {#sec-cpp-api-typedefs}

We define a shorthand for the maximum value of an unsigned integer.

```{cpp}
unsigned int uint_max = std::numeric_limits<unsigned int>::max();
```

The following typedef represents an indicator function. 

```{cpp}
typedef std::function<bool(double x)> indicator;
```

The following typedef represents a weight function. 

```{cpp}
typedef std::function<double(double x, bool log)> weight_dfd;
```

Here, `x` is the argument of the weight function and `log` determines if the result is returned on the log-scale.

The following typedef represents a function that optimizes (i.e., maximizes or
minimizes) a weight function on a given interval.

```{cpp}
typedef std::function<double(
	const weight_dfd& w,  // <1>
	double lo,            // <2>
	double hi,            // <3>
	bool log              // <4>
)> optimizer;
```
1. A weight function.
2. Lower bound of the interval; may be `R_NegInf`.
3. Upper bound of the interval; may be `R_PosInf`.
4. Logical; `log = true` specifies that the result should be the optimal value
   $\log w(x^*)$, given on the log-scale. Otherwise, the result should be the
   optimal value $w(x^*)$ on the original scale.

The following enum specifies actions for when error conditions are encountered.

```{cpp}
enum class error_action {
	STOP,
	WARNING,
	MESSAGE,
	NONE
};
```
1. Throw an exception.
2. Emit a warning and proceed.
3. Print a message and proceed.
4. Do not take any of the above actions and proceed.

**TBD:** should we try to use `fntl::error_action` instead?

**TBD** Can we pass `optimize_hybrid` to `RealConstRegion` as the default and
clean up that part of the code?

## Finite Mixture Proposal {#sec-cpp-api-proposal}

The class `FMMProposal` represents a VWS proposal. 

### Class Definition {.unlisted}

`FMMProposal` has two template arguments: `T` is the data type for the
underlying distribution (e.g., `T = double` for univariate and real-valued) and
`R` is the type of the regions of which the proposal will be composed.

```{cpp}
template <class T, class R>
class FMMProposal {
	...
}
```

### Constructor {.unlisted}

The primary constructors takes a vector of regions of type `R`. There must be
at least one region in the vector, and these regions are expected to be a
partition of the support $\Omega$.

```{cpp}
FMMProposal(const std::vector<R>& regions);
```

A second constructor takes a single region. This is a special case of the
previous one which is provided for convenience.

```{cpp}
FMMProposal(const R& region);
```

### Distribution Methods {.unlisted}

The following functions make use of the proposal as a distribution. They are
especially utilized in rejection sampling.

```{cpp}
std::vector<T> r(unsigned int n = 1) const;                                           // <1>
std::pair<std::vector<T>, std::vector<unsigned int>> r_ext(unsigned int n = 1) const; // <2>
double d(const T& x, bool normalize = true, bool log = false) const;                  // <3>
double w_major(const T& x, bool log = true) const;                                    // <4>
double d_target_unnorm(const T& x, bool log = true) const;                            // <5>
```
1. Draw $n$ variates of type `T` from the proposal.
2. Draw $n$ variates of type `T` from the proposal and retain the indices of
the regions used in each draw. The results are returned as an STL pair whose
first element is the vector of draws and second element is the vector of
indices.
3. Evaluate the density $h$ on the given $x$. If `normalize = false`, $h_0(x)$
is computed; otherwise $h(x)$ is computed. If `log = true`, results are given
on the log-scale.
4. Evaluate the majorized weight function $\overline{w}(x)$. If `log = true`,
results are returned on the log-scale.
5. Evaluate the unnormalized target $f_0(x) = w(x) g(x)$. If `log = true`, the
value on the log-scale is returned.

### Accessors {.unlisted}

The following accessors are provided.

```{cpp}
Rcpp::NumericVector xi_upper(bool log = true) const;                  // <1>
Rcpp::NumericVector xi_lower(bool log = true) const;                  // <2>
Rcpp::LogicalVector bifurcatable() const;                             // <3>
Rcpp::NumericVector pi(bool log = false) const;                       // <4>
Rcpp::NumericVector rejection_bound_regions(bool log = false) const;  // <5>
double rejection_bound(bool log = false) const;                       // <6>
double nc(bool log = false) const;                                    // <7>
unsigned int size() const;                                            // <8>
```
1. Get the constants $\overline{\xi}_1, \ldots, \overline{\xi}_N$.
2. Get the constants $\underline{\xi}_1, \ldots, \underline{\xi}_N$.
3. Get a vector of $N$ logical values indicating whether the corresponding
regions can be bifurcated. For example, when the support `T` is `int`, a
region $[a,b]$ containing one integer should not be bifurcated because one of
the two resulting regions will not contain any points of the support.
4. Get the mixing proportions $\pi_1, \ldots, \pi_N$.
5. Get the contributions $\rho_1, \ldots, \rho_N$ to bound \eqref{eqn:bound}
for each region.
6. Get the overall rejection bound \eqref{eqn:bound}.
7. Get the normalizing constant $\psi_N$
8. Get the number of regions $N$.

Methods above with the a `log` argument return values on the log-scale when
`log = true`.

### Iterator Methods {.unlisted}

The following methods can be used to get (read-only) iterators to internal data structures. These can be more efficient than the accessors above because they
do not make a copy of the data.

```{cpp}
std::set<R>::const_iterator regions_begin() const;               // <1>
std::set<R>::const_iterator regions_end() const;
Rcpp::NumericVector::const_iterator log_xi_upper_begin() const;  // <2>
Rcpp::NumericVector::const_iterator log_xi_upper_end() const;
Rcpp::NumericVector::const_iterator log_xi_lower_begin() const;  // <3>
Rcpp::NumericVector::const_iterator log_xi_lower_end() const;
Rcpp::LogicalVector::const_iterator bifurcatable_begin() const;  // <4>
Rcpp::LogicalVector::const_iterator bifurcatable_end() const;
```
1. The start and end of the set of regions (of template type `R`) in the proposal.
2. The start and end of the vector
$(\overline{\xi}_1, \ldots, \overline{\xi}_N)$.
3. The start and end of the vector
$(\underline{\xi}_1, \ldots, \underline{\xi}_N)$.
4. The start and end of the vector of indicators for whether regions are bifurcatable.

### Methods to Refine the Proposal {.unlisted}

Two functions are provided to refine the proposal from $\mathscr{D}_1, \ldots, \mathscr{D}_N$ into a finer partition. Both variants return a vector which represents values of the bound \eqref{eqn:bound} at each refinement step; the vector is of length $N_0 + 1$ where $N_0$ are the number of steps taken, the $j$th element represents the value at refinement $j = 0, \ldots, N_0$, and the element with index $j = 0$ represents the initial value. The first variant partitions at a given vector of knots. 

```{cpp}
Rcpp::NumericVector refine(
	const std::vector<T>& knots, // <1>
	bool log = true              // <2>
);
```
1. A vector of knots.
2. If `log = true` return bound values on the log-scale.

The second variant uses the rule of thumb for sequential knot selection
from @VWS2025. Refining will halt when \eqref{eqn:bound} reduces below `tol`;
this has an effect when `tol` is positive. Otherwise, `N` is the maximum number
of partition steps taken.

```{cpp}
Rcpp::NumericVector refine(
	unsigned int N,                  // <1>
	double tol = 0,                  // <2>
	bool greedy = false,             // <3>
	unsigned int report = uint_max,  // <4>
	bool log = true                  // <5>
);
```
1. Number of refinements to make.
2. Tolerance for \eqref{eqn:bound}; refinement will halt if this is reached.
3. If `greedy = true`, the region with the largest $\rho_\ell$ is always selected for partitioning; otherwise, regions are selected with probabilities proportional to $\rho_1, \ldots, \rho_N$.
4. If `log = true` return bound values on the log-scale.
5. The period at which progress is written to the console. E.g., use `period = 2` to report progress every two selections.

The `seq` function is provided as a convenience to generate equally-spaced knots for univariate real-valued intervals.

```{cpp}
std::vector<double> seq(double lo, double hi, unsigned int N, bool endpoints = false);
```

**TBD:** Should the first refine function take a vector of ints for `IntConstRegion` regions? Seems okay to leave it as is ...

### Summary Methods {.unlisted}

Several methods are provided to summarize the regions in the proposal.

```{cpp}
Rcpp::DataFrame summary() const;       // <1>
void print(unsigned int n = 5) const;  // <2>
```
1. Get a data frame with the summary.
1. Print summary to the console.

## Region Base Class {#sec-cpp-api-region}

`Region` is an abstract base class whose interface represents the
problem-specific logic that must be coded to implement VWS. Users create a
subclass of this method to construct a proposal for a given problem. However,
for the most common application of VWS - univariate support with a constant
majorizer - users may start with a specialized subclass. See
Sections [-@sec-cpp-api-realconstregion] and [-@sec-cpp-api-intconstregion].

**TBD**: mention that bifurcated objects inherit values of the original such
as `w` and the optimization functions.

### Class Definition {.unlisted}

The class has one template argument `T`, which is the data type for the
underlying distribution.

```{cpp}
template <class T>
class Region { ... }
```

### Public Methods {.unlisted}

The interface consists of the following public methods. These are abstract and
must be implemented in a subclass.

```{cpp}
virtual double d_base(const T& x, bool log = false) const = 0; // <1>
virtual std::vector<T> r(unsigned int n) const = 0;            // <2>
virtual bool s(const T& x) const = 0;                          // <3>
virtual double w(const T& x, bool log = true) const = 0;       // <4>
virtual double w_major(const T& x, bool log = true) const = 0; // <5>
virtual bool is_bifurcatable() const = 0;                      // <6>
virtual double xi_upper(bool log = true) const = 0;        // <7>
virtual double xi_lower(bool log = true) const = 0;        // <8>
virtual std::string description() const = 0;                   // <9>
```
1. Evaluate the density function $g$ of the base distribution.
2. Generate a vector of $n$ draws from $g_j$ specific to this region.
3. Indicator of whether $x$ is in the support for $g_j$ specific to this region.
4. The weight function $w$.
5. Majorized weight function $\overline{w}_j$ for this region.
6. Indicator of whether this region is bifurcatable into two smaller regions.
This is used when refining a proposal; see Section TBD. One reason that a
region should not be bifurcated is when one of the resulting regions will not
have any points of support.
7. The quantity $\overline{\xi}_j$ for this region.
8. The quantity $\underline{\xi}_j$ for this region.
9. A string that describes this region.

The argument `log = true` in the methods above requests values to be returned
on the log-scale.

## Region on Real-Valued Support with Constant Majorizer {#sec-cpp-api-realconstregion}

This is a subclass of `Region`, defined in @sec-cpp-api-region, specifically for
univariate problems with continuous support where
$\overline{w}(x) = \sum_{j=1}^N \overline{w}_j \ind\{x \in \mathscr{D}_j\}$ is
constructed from constants $\overline{w}_1, \ldots, \overline{w}_N$. Similarly,
a minorizer
$\underline{w}(x) = \sum_{j=1}^N \underline{w}_j \ind\{x \in \mathscr{D}_j\}$
is constructed from constants $\underline{w}_1, \ldots, \underline{w}_N$.
The $\overline{w}_j$ and $\underline{w}_j$ are obtained using numerical
optimization; however, if a closed-form solution is known, the user may
create a subclass and override the optimization method.

### Constructors {.unlisted}

In addition to the methods defined in `Region`, we have the following
constructors.

```{cpp}
RealConstRegion(
	double a,                                 // <1>
	double b,                                 // <2>
	const weight_dfd& w,                      // <3>
	const UnivariateHelper& helper,           // <4>
    const optimizer& maxopt = maxopt_default, // <5>
    const optimizer& minopt = minopt_default  // <6>
);

RealConstRegion(
	double a,                                 // <1>
	const weight_dfd& w,                      // <3>
	const UnivariateHelper& helper            // <4>
    const optimizer& maxopt = maxopt_default, // <5>
    const optimizer& minopt = minopt_default  // <6>
);
```
1. Lower limit of interval that defines this region.
2. Upper limit of interval that defines this region.
3. Weight function $w$ for the target distribution.
4. A container with operations of the base distribution $g$.
5. A function of type `optimizer` that maximizes `w` on the given region.
5. A function of type `optimizer` that minimizes `w` on the given region.

The first constructor creates a region based on the interval $(a,b]$; the
second creates a region based on the singleton set $\{a\}$, which is intended
primarily for internal use.

The default optimizers, `maxopt_default` and `minopt_default`, use the hybrid
numerical optimization method in @sec-cpp-api-opt to optimize `w`.

### Methods {.unlisted}

The `optimize` method maximizes or minimizes the weight function $w$ over the
given region. The the optimized value of $w$ is returned. If `maximize = true`
do maximization; otherwise do minimization. If `log = true`, return value on
the log-scale. Otherwise, return it on the original scale.

```{cpp}
double optimize(bool maximize = true, bool log = true) const;
```

**TBD:** Do we still have the `optimize` method? If so, do we need it?

The `midpoint` method returns a point between endpoints $a$ and $b$ of the
region. If $a$ and $b$ are both finite, return the standard midpoint. If both
are infinite, zero is returned. If only $a$ is finite, return a larger point in
the support. If only $b$ is finite, return a smaller point in the support.

```{cpp}
double midpoint() const;
```

The `bifurcate` method returns two disjoint regions whose union is the current
region. The result is given as an STL pair. The first version bifurcates at the
midpoint of the current region, determined by the `midpoint` method. The second
version partitions at the given $x$.

```{cpp}
std::pair<RealConstRegion,RealConstRegion> bifurcate() const;
std::pair<RealConstRegion,RealConstRegion> bifurcate(const double& x) const;
```

The `singleton` method returns a singleton interval $(x, x]$, using the current
object's weight function, base distribution, etc.

```{cpp}
RealConstRegion singleton(const double& x) const;
```

The following methods determine an ordering of the current region and an
another region specified as argument `x`. Region $(a_1, b_1]$ is considered
"less than" $(a_2, b_2]$ if $b_1 < a_2$. The regions are considered equal if
$a_1 = a_2$ and $b_1 = b_2$. Note that other elements such as $w$ are $g$ are
not explicitly checked, and are assumed to be the same.

```{cpp}
bool operator<(const RealConstRegion& x) const;
bool operator==(const RealConstRegion& x) const;
```

The following method assigns the current region to be equal to the argument `x`.

```{cpp}
const RealConstRegion& operator=(const RealConstRegion& x);
```

## Region on Integer-Valued Support with Constant Majorizer  {#sec-cpp-api-intconstregion}

**TBD**: do we still need this?

## Univariate Helper {#sec-cpp-api-helper}

`UnivariateHelper` is a class which is intended for use with `RealConstRegion`. It encapsulates several operations needed from the base distribution $g$. These operations are specified as lambdas in the constructor.


```{cpp}
UnivariateHelper(
	const fntl::density& d,  // <1>
	const fntl::cdf& p,      // <2>
	const fntl::quantile& q, // <3>
	const supp& s            // <4>
);
```
1. A function to evaluate density $g$.
2. A function to evaluate the CDF $G$.
3. A function to evaluate the quantile function $G^{-}$.
4. An indicator function that returns $1$ if its argument is in the support of
$g$; otherwise, it returns $0$.

The following methods on `UnivariateHelper` utilize the lambdas specified above.

```{cpp}
double d(double x, bool log = false) const;                     // <1>
double p(double q, bool lower = true, bool log = false) const;  // <2>
double q(double p, bool lower = true, bool log = false) const;  // <3>
bool s(double x) const;                                         // <4>
```
1. Evaluate the density function at argument $x$. Result is on the log-scale if
`log = true`.
2. Evaluate the cumulative distribution function (CDF) at argument $q$. Result
is on the log-scale if `log = true`. Result represents $P(X \leq q)$ if
`lower = true` and $P(X > q)$ otherwise.
3. Evaluate the quantile function at argument $p$. Assume $p$ is specified on
the log-scale if `log = true`. Request $p$ quantile if `lower = true` and
$1-p$ quantile otherwise.
4. Indicator of whether argument $x$ is in the support of the distribution.


## Rejection Sampling {#sec-cpp-api-rejection}

The following functions carry out rejection sampling using a VWS proposal
described in @sec-cpp-api-proposal.

```{cpp}
template <typename T, typename R>
inline rejection_result<T> rejection(
	const FMMProposal<T,R>& h,         // <1>
	unsigned int n,                    // <2>
	const rejection_args& args         // <3>
);

template <typename T, typename R>
inline rejection_result<T> rejection(
	const FMMProposal<T,R>& h,         // <1>
	unsigned int n                     // <2>
);
```
1. An `FMMProposal` object to use as the proposal.
2. The number of desired draws.
3. Additional arguments for rejection sampling. Default values are assumed in
the second form.

Template arguments `T` and `R` correspond to the given proposal `h` and are
described in @sec-cpp-api-proposal. Additional arguments are provided via the
following struct.

```{cpp}
struct rejection_args
{
	unsigned int max_rejects = std::numeric_limits<unsigned int>::max(); // <1>
	unsigned int report = std::numeric_limits<unsigned int>::max();      // <2>
	double ratio_ub = std::exp(1e-5);                                    // <3>
	error_action action = error_action::STOP;                            // <4>

	rejection_args() { };                                                // <5>
	rejection_args(SEXP obj);                                            // <6>
	operator SEXP() const;                                               // <7>
};
```
1. Maximum number of rejections to tolerate overall (among all $n$ attempted
draws) before bailing out.
2. Determines period at which progress is logged to the console.
3. Upper bound for the ratio $f_0(x) / h_0(x)$. The ratio may be slightly
larger than 1 due to numerical precision, but an error is thrown if it is
larger than this value.
4. The action to take if `max_rejects` rejections is exceeded. See
below for possible values.
5. Constructor that takes no arguments.
6. Convert an `Rcpp::List` to a `rejection_args` struct.
7. Return a `Rcpp::List` from a `rejection_args` struct.

The return value of `rejection` is a struct of the following type.

```{cpp}
template <typename T>
struct rejection_result
{
	std::vector<T> draws;               // <1>
	std::vector<unsigned int> rejects;  // <2>

	operator SEXP() const;              // <3>
};
```
1. Vector of draws.
2. Vector of rejection counts; the $i$th element represents number of
rejections observed before accepting the $i$th draw.
3. Return a `Rcpp::List` from a `rejection_args` struct.

If `action != error_action::STOP` in `rejection_args`, the sampler can halt
before all $n$ desired variates are drawn. In this case, the `draws` and
`rejects` vectors will have lengths shorter than $n$.

**TBD** here is `error_action`.


## Optimization on an Interval {#sec-cpp-api-opt}

A hybrid optimization method for univariate functions $f(x) : [a,b] \rightarrow \mathbb{R}$ with bounds $x \in [a,b]$ that may be infinite. Uses Brent's method if both bounds are finite and BFGS otherwise. In the latter case, the bounds are enforced via a transformation.

```{cpp}
optimize_hybrid_result optimize_hybrid(
	const fntl::dfd& f,       // <1>
    double init,              // <2>
	double lower,             // <3>
	double upper,             // <4>
	bool maximize,            // <5>
	unsigned maxiter = 100000 // <6>
);
```
1. Objective function.
2. Initial value used with BFGS.
3. Lower bound.
4. Upper bound.
5. Logical; if `true`, optimization will be a maximization. Otherwise it is a minimization.
6. Maximum number of iterations.

The result is a `optimize_hybrid_result` struct which is defined as follows.

```{cpp}
struct optimize_hybrid_result {
	double par;            // <1>
	double value;          // <2>
	std::string method;    // <3>
	int status;            // <4>

	operator SEXP() const; // <5>
};
```
1. Final value of the optimization variable $x$.
2. Final value of the objective function $f(x)$.
3. Description of the method used to find the result; see below.
4. Corresponds to a code from BFGS if it is used as `method`; otherwise zero.
5. Return an `Rcpp::List` from a `optimize_hybrid_result` struct.

The `method` field can take on the following values:

- "Brent": Brent optimization method was used.
- "BFGS": BFGS method was used.
- "Lower Limit Inf": For a maximization problem, the lower limit was taken
  as `par` because it had value `inf`.
- "Upper Limit Inf": For a maximization problem, the upper limit was taken
  as `par` because it had value `inf`.
- "Lower Limit NegInf": For a minimization problem, the lower limit was
  taken as `par` because it had value `-inf`.
- "Upper Limit NegInf": For a minimization problem, the upper limit was
  taken as `par` because it had value `-inf`.
- "Max at Lower Limit": numerical maximization was used, but a larger value
  was found at the lower limit.
- "Max at Upper Limit": numerical maximization was used, but a larger value
  was found at the upper limit.
- "Min at Lower Limit": numerical minimization was used, but a smaller value
  was found at the lower limit.
- "Min at Upper Limit": numerical minimization was used, but a smaller value
  was found at the upper limit.


## Log-Scale Arithmetic {#sec-cpp-api-logscale}

Calculations in the package are carried out on the log-scale, where possible,
to avoid issues from floating point numbers with very small or very large
magnitudes. Users may want to follow this convention when implementing their
own sampling problems. Several included functions help to avoid explicit
exponentiation and may be helpful for this purpose.

The following computes the scalar $f(x) = \log\{ \sum_{i=1}^n \exp(x_i) \}$ from a vector $x \in \mathbb{R}^n$.

```{cpp}
double log_sum_exp(const Rcpp::NumericVector& x);
```

The following compute addition on the log scale: $\log(e^x + e^y)$. The first
form takes scalar $x$ and $y$. The second and third forms take
$x, y \in \mathbb{R}^n$ and produce an $n$-dimensional vector.

```{cpp}
double log_add2_exp(double x, double y);
std::vector<double> log_add2_exp(const std::vector<double>& x, const std::vector<double>& y);
Rcpp::NumericVector log_add2_exp(const Rcpp::NumericVector& x, const Rcpp::NumericVector& y);
```

The following carry out subtraction on the log scale: $\log(e^x - e^y)$. The
first form takes scalar $x$ and $y$. The second and third forms take
$x, y \in \mathbb{R}^n$ and produce an $n$-dimensional vector. Here, elements of $x$ smaller than $y$ result in `NaN`.

```{cpp}
double log_sub2_exp(double x, double y);
std::vector<double> log_sub2_exp(const std::vector<double>& x, const std::vector<double>& y);
Rcpp::NumericVector log_sub2_exp(const Rcpp::NumericVector& x, const Rcpp::NumericVector& y);
```

## Generating from a Discrete Distribution {#sec-cpp-api-discrete}

We make use of the Gumbel trick [e.g., @HuijbenEtAl2023] to draw from a discrete distribution with probabilities $p_1, \ldots, p_k$. This approach allows probabilities to be specified on the log-scale without the need to exponentiate or normalize them so that they sum to one. The Gumbel trick generates a draw $x$ from the desired discrete distribution via
<!-- -->
$$
X = \argmax \{ Z_1 + \log p_1, \ldots, Z_k + \log p_k \},
\quad Z_1, \ldots, Z_k \iid \text{Gumbel}(0,1),
$$
<!-- -->
where $\text{Gumbel}(0,1)$ is a standard Gumbel distribution with density
$f(x) = e^{-(x + e^{x})}$. A benefit of this method is that the probabilities
can be given on the log-scale and do not necessarily need to be exponentiated.

### Categorical Distribution {.unlisted} 

The following functions generate a draw of $X$. The first form generates a
single variate and the second form generates a sample of size $n$.

```{cpp}
unsigned int r_categ(
	const Rcpp::NumericVector& p,  // <2>
	bool log = false,              // <3>
	bool one_based = false         // <4>
);
Rcpp::IntegerVector r_categ(
	unsigned int n,                // <1>
	const Rcpp::NumericVector& p,  // <2>
	bool log = false,              // <3>
	bool one_based = false         // <4>
);
```
1. Desired sample size.
2. Vector of probabilities $p_1, \ldots, p_k$.
3. Logical; if `true`, indicates that argument `p` should be interpreted as
$\log p_1, \ldots, \log p_k$. Otherwise, it is interpreted as
$p_1, \ldots, p_k$ on the original scale.
4. Logical; if `true`, support is assumed to be $1, \ldots, k$, where $k$ is
length of the given `p`. Otherwise it is assumed to be $0, \ldots, k-1$. The
former is useful to generate indices in C++ while the latter is useful for
indices in R.

### Gumbel Distribution {.unlisted}

The following functions are provided for the Gumbel distribution with location
parameter $\mu$ and scale $\sigma$. They provide density, CDF, quantile, and
variate generation, respectively.

```{cpp}
double d_gumbel(double x, double mu = 0, double sigma = 1,
	bool log = false);
double p_gumbel(double q, double mu = 0, double sigma = 1,
	bool lower = true, bool log = false);
double q_gumbel(double p, double mu = 0, double sigma = 1,
	bool lower = true, bool log = false);
double r_gumbel(double mu = 0, double sigma = 1);
```

The following vectorized versions operate on an independent and identically
distributed sample.

```{cpp}
Rcpp::NumericVector d_gumbel(const Rcpp::NumericVector& x, double mu = 0,
	double sigma = 1, bool log = false);
Rcpp::NumericVector p_gumbel(const Rcpp::NumericVector& q, double mu = 0,
	double sigma = 1, bool lower = true, bool log = false);
Rcpp::NumericVector q_gumbel(const Rcpp::NumericVector& p, double mu = 0,
	double sigma = 1, bool lower = true, bool log = false);
Rcpp::NumericVector r_gumbel(unsigned int n, double mu = 0, double sigma = 1);
```

# Example: Von Mises Fisher {#sec-vmf}

Let us consider generating from the von Mises Fisher (VMF) distribution as in @VWS2025. VMF is useful in modeling directional data whose support is the sphere $\mathbb{S}^{d-1} = \{ \vec{v} \in \mathbb{R}^d : \vec{v}^\top \vec{v} = 1 \}$. A random variable $\vec{V}$ with distribution $\text{VMF}_d(\vec{\mu}, \kappa)$ has density
<!-- -->
\begin{align*}
f_{\text{VMF}}(\vec{v}) = \frac{
\kappa^{d / 2 - 1}
}{
(2 \pi)^{d/2} I_{d/2 - 1}(\kappa)
} \exp(\kappa \cdot \vec{\mu}^\top \vec{v}) \cdot \ind\{\vec{v} \in \mathbb{S}^{d-1}\},
\end{align*}
<!-- -->
with modified Bessel function of the first kind
<!-- -->
\begin{math}
I_{\nu}(x) = \sum_{m=0}^\infty \{m! \cdot \Gamma(m + \nu + 1)\}^{-1} (\frac{x}{2})^{2m + \nu}
\end{math}
<!-- -->
and gamma function $\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt$. Parameters $\vec{\mu} \in \mathbb{S}^{d-1}$ and $\kappa > 0$ determine the orientation on the sphere and the concentration, respectively. First consider $\vec{\mu}_0 = (1, 0, \ldots, 0)$. A random variable $\vec{V}_0 \sim \text{VMF}_d(\vec{\mu}_0, \kappa)$ can be obtained from the transformation
<!-- -->
\begin{align}
\vec{V}_0 = \left( X, \sqrt{1 - X^2} \cdot \vec{U} \right),
\label{eqn:vmf-tx}
\end{align}
<!-- -->
where $\vec{U}$ is a uniform random variable on the sphere $\mathbb{S}^{d-2}$ and $X$ has density
<!-- -->
\begin{align}
f(x) =
\frac{
(\kappa / 2)^{d/2 - 1} (1 - x^2)^{(d-3)/2} \exp(\kappa x)
}{
\sqrt{\pi} \cdot I_{d/2 - 1}(\kappa) \cdot \Gamma((d-1)/2)
} \cdot \ind\{-1 < x < 1\}.
\label{eqn:vmf-target}
\end{align}
<!-- -->
To obtain a draw from $\vec{V} \sim \text{VMF}_d(\vec{\mu}, \kappa)$ with an arbitrary $\vec{\mu}$, we can rotate $\vec{V} = \vec{Q} \vec{V}_0$ using an orthonormal matrix $\vec{Q}$ whose first column is $\vec{\mu}$. In the construction $\vec{V}_0$ from \eqref{eqn:vmf-tx}, we may draw $\vec{U} = \vec{Z} / \sqrt{\vec{Z}^\top \vec{Z}}$ from $\vec{Z} \sim \text{N}(\vec{0}, \vec{I}_{d-1})$ and $X$ independently from \eqref{eqn:vmf-target}. In the following, we consider the use of VWS to draw the univariate random variable $X$. Before proceeding, we give a notation to a useful distribution.

::: {#def-texp}
Denote $X \sim \text{Exp}_{(a,b)}(\kappa)$ to as a doubly truncated Exponential random variable with density
<!-- -->
\begin{align*}
g(x) = \frac{\kappa e^{\kappa x}}{e^{\kappa b} - e^{\kappa a}} \cdot \ind\{a < x < b\},
\end{align*}
<!-- -->
where $-\infty < a < b < \infty$ and rate $\kappa$ may be any real number. The CDF and quantile function corresponding to $g$ are
<!-- -->
\begin{align*}
&G(x) = \frac{e^{\kappa x} - e^{\kappa a}}{e^{\kappa b} - e^{\kappa a}} \cdot,
\quad x \in (a, b), \\
%
&G^{-1}(\varphi) = \frac{1}{\kappa} \log\left[e^{\kappa a} + \varphi (e^{\kappa b} - e^{\kappa a}) \right], \quad \varphi \in [0,1].
\end{align*}
:::

Returning to target \eqref{eqn:vmf-target}, let us decompose $f$ into
<!-- -->
\begin{align*}
f(x) \propto
\underbrace{(1 - x^2)^{(d-3)/2}}_{w(x)}
\underbrace{\exp(\kappa x) \cdot \ind\{x \in (-1,1)\}}_{g_0(x)},
\end{align*}
<!-- -->
so that $w$ is the weight function and $g_0$ is proportional to the density of $\text{Exp}_{(-1,1)}(\kappa)$,
<!-- -->
\begin{align*}
g(x) = \frac{\kappa e^{\kappa x}}{e^\kappa - e^{-\kappa}} \cdot \ind\{x \in (-1,1)\}.
\end{align*}
<!-- -->
@sec-vmf-const-default obtains a VWS sampler with this decomposition using a constant majorizer. @sec-vmf-const-custom replaces the default numerical optimization with custom code, which reduces the amount of computational overhead. @sec-vmf-linear considers a linear majorizer which is substantially more involved but also obtains substantially lower rejection rates with a moderate number of regions. Codes for this example are in the folder `examples/vmf`. C++ functions for the $\text{Exp}_{(a,b)}(\kappa)$ distribution from @def-texp are given in the file `examples/vmf/texp.h`.

::: {#rem-vmf-caveat}
A caveat of this decomposition is that, in the $d < 3$ case, $w(x) \rightarrow \infty$ as $x$ approaches $\pm 1$. One way to avoid this is by truncating the support to $(\alpha_0, \alpha_N] = (-1 + \epsilon, 1 - \epsilon]$ for a small $\epsilon > 0$. Rejection sampling can proceed using the truncated support if the exclusion of $(-1, -1 + \epsilon] \cup (1 - \epsilon, 1]$ is known to have a negligible impact on the result. Otherwise, @VWS2025 mention another strategy where the support is initially truncated and gradually expanded as rejections are encountered. In this document, we assume $(\alpha_0, \alpha_N] = (-1, 1]$ for $d > 3$ and a fixed truncation $(\alpha_0, \alpha_N] = (-1 + \epsilon, 1 - \epsilon]$ for $d < 3$.

**TBD: left off here**
:::

## Constant Majorizer with Numerical Optimization {#sec-vmf-const-default}

We now give our first example demonstrating the `vws` package. We consider a constant majorizer for $w$ which uses the default numerical optimization routine to identify appropriate constants $\overline{w}_j$ and $\underline{w}_j$. Because this is our first example, the source file for the sampler (`examples/vmf/vmf-v1.cpp`) is displayed in its entirety as follows.

```{.cpp include="examples/vmf/vmf-v1.cpp" code-line-numbers="true"}
```

There are several points of interest to call out.

- Line 1 ensures that this code links with the `vws` and `fntl` packages during
  compilation.
- Line 2 includes the header for C++ framework in `vws`.
- Line 3 includes `texp.h`, which provides functions described in @def-texp.
- Lines 5-7 define a C++ function which invokes the sampler and exports it for
  use in R.
- Lines 9-11 prepare a struct with extra arguments for rejection sampling.
  Notice that `rejection_args`, along with other `vws` classes and functions,
  are in the `vws` namespace.
- Lines 13-20 define the weight function using C++ lambda syntax. We are
  careful to avoid `nan` values that can occur when $x = \pm 1$. Computations
  are carried out on the log-scale to avoid numerical loss of precision.
- Lines 22-31 specify the density, CDF, and quantile function of the base
  distribution $\text{Exp}_{(-1,1)}(\kappa)$. These are defined as lambdas
  whose signatures are defined in the `fntl` package. For example, a function
  of type `fntl::density` takes two inputs: a double `x` which is the the
  argument and `log` which is a boolean that specifies whether the value should
  be returned on the log-scale.
- Line 32 creates a "helper" object as a container for the distribution
  functions.
- Line 33 constructs a specific type of `Region`, which contains all
  problem-specific logic of the sampler. In this case, it is a
  `RealConstRegion` which facilitates construction for continuous univariate
  distributions and constant majorizers. We construct one region here which
  contains the entire support $(-1,1]$.
- Line 34 constructs a `FMMProposal` based on our initial region `supp`.
  Notice that there are two template arguments specified here: the
  first specifies that the data type of the support is `double`; the second
  specifies that regions (which include the logic of the sampler) are of type
  `RealConstRegion`.
- Line 36 requests the proposal `h` to refine itself `N-1` times so that there
  are `N` regions.
- Line 37 carries out rejection sampling with proposal `h`.
- Lines 39-43 assemble an `Rcpp::List` to return to the called. It contains 
  the draws in element `draws`, a vector of rejection counts in `rejects` where
  the `i`-th element represents the number of rejections for the `i`-th draw,
  and a vector in element `lbdd` with the $N-1$ bounds
  \eqref{eqn:bound-components} achieved at each of the refinement steps
  (returned on the log-scale).

The name of the function `r_vmf_pre_v1` reflects that this is our first version of the sampler for target \eqref{eqn:vmf-target}, which is a precursor to transformation \eqref{eqn:vmf-tx} to obtain a VMF random variable. We may invoke the exported R function as follows.

```{r}
Rcpp::sourceCpp("examples/vmf/vmf-v1.cpp")
out1 = r_vmf_pre_v1(n = 1000, kappa = 5, d = 4, N = 50, tol = 0.10)
head(out$draws)
```

@fig-vmf-const-refine plots the bound for the rejection probability during the refinement process, which is captured in the variable `out$lbdd`.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Rate of refine improvement for VMF example with constant majorizer.
#| label: fig-vmf-const-refine
#| echo: false

data.frame(bdd = exp(out1$lbdd)) %>%
	mutate(step = row_number() - 1) %>%
	ggplot() +
	geom_line(aes(x = step, y = bdd)) +
	scale_y_continuous(n.breaks = 10) +
	xlab("Step") +
	ylab("Bound") +
	theme_minimal()
```

@fig-vmf-const-draws plots the empirical distribution of the draws and compares
them to the density.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Empirical distribution of draws versus target for VMF example with
#|   constant majorizer. The target density is displayed for reference as the
#|   dashed curve.
#| label: fig-vmf-const-draws
#| echo: false

data.frame(x = out1$draws) %>%
	ggplot() +
	geom_density(aes(x = x), col = "black") +
	geom_function(fun = d_target, args = list(kappa = 5, d = 4), lty = 2) +
	ylab("Empirical Density") +
	theme_minimal()
```

## Constant Majorizer with Custom Optimization {#sec-vmf-const-custom}

It is not difficult to find the min and max of the function $w$ on a given interval for this example. We can reduce some of the computational burden by providing functions to compute these two values.

We have
<!-- -->
\begin{align*}
&\log w(x) = \frac{d-3}{2} \log(1 - x^2), \\
&\frac{d}{dx} \log w(x) = -(d-3) \frac{x}{1 - x^2}, \\
&\frac{d^2}{dx^2} \log w(x) = -(d-3) \frac{1 + x^2}{(1 - x^2)^2}.
\end{align*}
<!-- -->
Therefore, it is seen that $\log w(x)$ is concave when $d > 3$, convex when $d = 2$, and constant otherwise. When $d > 3$, $\frac{d}{dx} \log w(x)$ is positive for $x \in (-1, 0)$, negative for $x \in (0, 1)$, and has root $x = 0$; therefore,  $\log w(x)$ is unimodal on $(-1, 1)$ with a maximum at $x = 0$. When $d = 2$, the point $x = 0$ is instead a minimum of $\log w(x)$. Finally, $w$ is a constant in the case $d = 3$. We can code these maximization and minimization routines as lambdas.

```{cpp}
vws::optimizer opt1 = [&](const vws::weight_dfd& w, double lo, double hi, bool log)
{
	double out;
	if (lo <= 0 && 0 < hi) {
		out = w(0, true);
	} else if (hi < 0) {
		out = w(hi, true);
	} else {
		out = w(lo, true);
	}
	return log ? out : std::exp(out);
};
```

```{cpp}
vws::optimizer opt2 = [&](const vws::weight_dfd& w, double lo, double hi, bool log)
{
	double w_lo = w(lo, true);
	double w_hi = w(hi, true);
	double out = std::min(w_lo, w_hi);
	return log ? out : std::exp(out);
};
```

The following snippet creates pointers `maxopt` and `minopt` for the maximizer and minimizer functions. The condition $d > 3$ is checked to determine which is the maximizer and which is the minimizer.

```{cpp}
vws::optimizer* maxopt;
vws::optimizer* minopt;
if (d >= 3) {
	maxopt = &opt1;
	minopt = &opt2;
} else {
	minopt = &opt1;
	maxopt = &opt2;
}
```

Finally, we create the initial region `supp`. The `maxopt` and `minopt`
functions are provided to the constructor as additional arguments. Note that
we dereference our pointers and pass the function objects themselves (which
are used as references by the constructor).

```{cpp}
vws::UnivariateHelper helper(df, pf, qf);
vws::RealConstRegion supp(-1, 1, w, helper, *maxopt, *minopt);
```

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/vmf/vmf-v2.cpp")
out2 = r_vmf_pre_v2(n = 1000, kappa = 5, d = 4, N = 50, tol = 0.10)
head(out2$draws)
```

## Linear Majorizer {#sec-vmf-linear}

**TBD: explain the construction of linear coefficients and the choice of $c$ in the background material at the top of the document.**

We noted in @sec-vmf-const-custom that $w$ is log-convex when $d < 3$, log-concave when $d > 3$, and a constant otherwise. Therefore, we can majorize $w$ with exponentiated linear functions of the form $\overline{w}_j(x) = \exp\{ \overline{\beta}_{j0} + \overline{\beta}_{j1} x \}$. This yields the expression
<!-- -->
\begin{align*}
\overline{\xi}_j
= \int_{\alpha_{j-1}}^{\alpha_j} \overline{w}_j(x) g(x) dx
= \frac{\kappa \exp\{ \overline{\beta}_{j0} \}}{(\kappa + \overline{\beta}_{j1}) (e^{\kappa \alpha_N} - e^{\kappa \alpha_0})} \left\{
\exp\{(\kappa + \overline{\beta}_{j1}) \alpha_j\} - \exp\{(\kappa + \overline{\beta}_{j1}) \alpha_{j-1} \}
\right\}.
\end{align*}
<!-- -->
The proposal $h$ is then a finite mixture $h(x) = \sum_{j=1}^N \pi_j g_j(x)$ with
<!-- -->
\begin{align*}
g_j(x)
&= \overline{w}_j(x) g(x) \ind\{ x \in \mathcal{D}_j \} / \overline{\xi}_j \\
&= \frac{
(\kappa + \overline{\beta}_{j1}) \exp\{(\kappa + \overline{\beta}_{j1}) x\}
}{
\exp\{(\kappa + \overline{\beta}_{j1}) \alpha_j\} - \exp\{(\kappa + \overline{\beta}_{j1}) \alpha_{j-1} \}
}
\cdot \ind(\alpha_{j-1} < x \leq \alpha_j),
\end{align*}
<!-- -->
the density of $\text{Exp}_{(\alpha_{j-1}, \alpha_j]}(\kappa + \overline{\beta}_{j1})$. To construct $\overline{\beta}_{j0}$ and $\overline{\beta}_{j1}$, we make use of method (**TBD**) using the expression
<!-- -->
\begin{align*}
M_j(s) = \int_{\alpha_{j-1}}^{\alpha_j} e^{sx} g(x) dx
= \frac{e^{s\alpha_j} - e^{s\alpha_{j-1}}}{s(\alpha_j - \alpha_{j-1})}.
\end{align*}
<!-- -->
To compute \eqref{eqn:bound}, we assume the "trivial" minorizer $\underline{w}_j(x) = w(x)$ so that
<!-- -->
\begin{align*}
\underline{\xi}_j
= \int_{\alpha_{j-1}}^{\alpha_j} w(x) g(x) dx
= \int_{\alpha_{j-1}}^{\alpha_j} \frac{(1-x^2)^{(d-3)/2} \kappa e^{\kappa x}}{e^{\kappa \alpha_N} - e^{\kappa \alpha_0}} dx.
\end{align*}

The proposal is implemented with the `vws` package by inheriting from the abstract `Region` base class (@sec-cpp-api-region) and implementing each of the functions using the expressions above. We name this subclass `LinearVWSRegion`; its complete code is given in `examples/vmf/LinearVWSRegion.h`. The code in `examples/vmf/vmf-v3.cpp` instantiates a proposal with regions of type `LinearVWSRegion`, invokes rejection sampling, and returns an `Rcpp::List` with the results. This code is displayed below.

```{.cpp include="examples/vmf/vmf-v3.cpp" code-line-numbers="true"}
```

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/vmf/vmf-v3.cpp")
out3 = r_vmf_pre_v3(n = 1000, kappa = 5, d = 4, N = 50, tol = 0.01)
head(out3$draws)
```

**TBD: do we need to truncate our (-1,1) support in the code? Check the implementation in VWS paper!!**

@fig-vmf-const-draws plots the empirical distribution of the draws and compares
them to the density.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Empirical distribution of draws versus target for VMF example with
#|   linear majorizer. The target density is displayed for reference as the
#|   dashed curve.
#| label: fig-vmf-linear-draws
#| echo: false

data.frame(x = out3$draws) %>%
	ggplot() +
	geom_density(aes(x = x), col = "black") +
	geom_function(fun = d_target, args = list(kappa = 5, d = 4), lty = 2) +
	ylab("Empirical Density") +
	theme_minimal()
```

@fig-vmf-refine plots the bound for the rejection probability for this sampler after each step of refining, along with those from the previous two versions. A substantial improvement in efficiency is seem here; fewer regions are needed to achieve a small rejection probability. Although versions 1 and 2 of the sampler are based on the same proposal, the changes to their bounds are seen to differ due to randomness in knot selection.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Rate of refine improvement for VMF example with constant majorizer.
#| label: fig-vmf-refine
#| echo: false

df1 = data.frame(sampler = "v1", bdd = exp(out1$lbdd)) %>% 
	mutate(N = row_number())
df2 = data.frame(sampler = "v2", bdd = exp(out2$lbdd)) %>% 
	mutate(N = row_number())
df3 = data.frame(sampler = "v3", bdd = exp(out3$lbdd)) %>% 
	mutate(N = row_number() + 1)
df = bind_rows(df1, df2, df3) %>% mutate(N = as.integer(N))

ggplot(df) +
	geom_line(aes(x = N, y = bdd, color = sampler), lwd = 1.02, alpha = 1) +
	geom_point(aes(x = N, y = bdd, pch = sampler, color = sampler),
		cex = 3, alpha = 1) +
	scale_x_continuous(breaks = seq(0, max(df$N))) +
	scale_y_continuous(n.breaks = 10) +
	xlab("N") +
	ylab("Bound") +
	theme_minimal()
```

# Example: Lognormal-Normal Conditional Distribution {#sec-ln-norm}

The setting $Z = Y + \gamma$ is the basis of a modeling scenario considered by @DirectSamplingDAS2021, @DPSimulation2022, and @DASMethods2025 where sensitive data are released under a measure of privacy protection. Here, $Y$ represents sensitive underlying data such as a tabulation of respondents' data collected by an official statistics agency and $\gamma$ is random noise added for privacy protection. The resulting $Z$ is considered protected and suitable for release. The objective is to carry out inference on $Y$ given an observed $Z = z$. The field of differential privacy studies mathematical criteria for privacy and the design of noise mechanisms which can satisfy those criteria [e.g., @DworkRoth2014]. @AbowdEtAl2022 describe work by the U.S. Census Bureau to implement differential privacy in the release of data from the decennial census. Our present motivation is to consider a simple but nontrivial sampling problem that arises in analysis of the released $z$; the interested reader is encouraged to see the given references as a starting point on privacy protection and differential privacy.

Suppose $Y$ and $\gamma$ are independently distributed with $Y \sim \text{Lognormal}(\mu, \sigma^2)$ and $\gamma \sim \text{N}(0, \lambda^2)$. The variance $\lambda^2$ is assumed to be known and provided with the noisy data. Such transparency into the noise mechanism is often a feature of differential privacy.

Suppose the target distribution is the conditional $[Y \mid Z = z, \mu, \sigma^2]$ whose density is given by
<!-- -->
\begin{align}
f(y \mid z, \mu, \sigma^2)
%
&\propto \frac{1}{\lambda \sqrt{2\pi}} \exp\left\{
-\frac{1}{2\lambda^2} (z - y)^2
\right\} \cdot
\frac{1}{y\sigma \sqrt{2\pi}} \exp\left\{
-\frac{1}{2\sigma^2} (\log y - \mu)^2
\right\} \ind\{y \in (0,\infty)\}
\nonumber \\
%
&\propto
\underbrace{\frac{1}{\lambda \sqrt{2\pi}} \exp\left\{
-\frac{1}{2\lambda^2} (z - y)^2
\right\}}_{g(y)} \cdot
\underbrace{\frac{1}{y} \exp\left\{
-\frac{1}{2\sigma^2} (\log y - \mu)^2
\right\} \ind\{y \in (0,\infty)\}}_{w(y)}.
\label{eqn:ln-norm}
\end{align}
<!-- -->
We have decomposed $f$ into weight function
<!-- -->
\begin{math}
w(y) = \frac{1}{y} \exp\left\{ -\frac{1}{2\sigma^2} (\log y - \mu)^2 \right\}
\ind\{ y \in (0,\infty) \},
\end{math}
<!-- -->
from the Lognormal component---dropping some of the terms in its normalizing constant---and base distribution $g$ as $\text{N}(z, \lambda^2)$. The conditional $[\vec{y} \mid \sigma^2, \vec{\mu}, \vec{z}]$ in \eqref{eqn:ln-norm} would be encountered in a Gibbs sampler for the posterior distribution $[\vec{y}, \vec{\mu}, \sigma^2 \mid \vec{z}]$, based on an observed sample $\vec{z} = (z_1, \ldots, z_n)$ with augmented data $\vec{y} = (y_1, \ldots, y_n)$. The remaining conditionals $[\vec{\mu} \mid \sigma^2, \vec{y}, \vec{z}]$ and $[\sigma^2 \mid \vec{\mu}, \vec{y}, \vec{z}]$ may be straightforward to generate if a convenient prior distribution is selected; therefore, we will focus on \eqref{eqn:ln-norm}. It will sometimes be more convenient (and more numerically stable) to work with $w$ on the log-scale; therefore, define
<!-- -->
\begin{align}
\zeta(y) = \log w(y)
= -\log y - \frac{(\log y - \mu)^2}{2 \sigma^2} + \log \ind\{ y \in (0,\infty) \}.
\label{eqn:ln-norm-log-weight-fn}
\end{align}

We will consider three variations of a VWS sampler, progressing from easier-to-implement to more computationally efficient. @sec-ln-norm-const-default considers a constant majorizer where the constant for each region is obtained by numerical optimization. @sec-ln-norm-const-custom replaces numerical optimization with code for a closed-form solution. @sec-ln-norm-linear makes use of a linear majorizer; this is somewhat more involved as it requires implementing a custom `Region`.

Before proceeding, let us fix the following values for the parameters.

```{r}
#| prompt: true
mu = 5
sigma = sqrt(0.5)
lambda = 10
```

Jointly draw values $Y$ and $Z$ from the model; $Z$ is considered observed while $Y$ is latent and the objective for inference.

```{r}
set.seed(1234)
y_true = rlnorm(1, mu, sigma)
z = rnorm(1, y_true, lambda)
print(y_true)
print(z)
```

Let us code the target density to assist in evaluating the distribution of the draws. To compute the normalizing constant, we will use Hermite quadrature via the `gauss.quad` function in the `statmod` package [@Smyth2005]. The integral $\psi = \int_{-\infty}^\infty q(x) e^{-x^2} dx$ is approximated as $\psi \approx \sum_{j=1}^Q \omega_j q(x_j)$ using quadrature points $x_1, \ldots, x_Q$ and weights $\omega_1, \ldots, \omega_Q$; to identify the function $q$, we have
<!-- -->
\begin{align*}
\psi &= \int_{-\infty}^\infty w(y) g(y) dy \\
%
&= \int_{-\infty}^\infty \ind\{y > 0\} \cdot \frac{1}{y}
\exp\left\{ -\frac{1}{2\sigma^2} (\log y - \mu)^2 \right\}
\frac{1}{\lambda \sqrt{2\pi}} \exp\left\{ -\frac{1}{2\lambda^2} (z - y)^2 \right\} dy \\
%
&= \int_{-\infty}^\infty \ind\left( z > \sqrt{2} \lambda x \right)
\cdot \frac{1}{z - \sqrt{2} \lambda x}
\exp\left\{ -\frac{1}{2\sigma^2} \left[\log\left( z - \sqrt{2} \lambda x \right) - \mu \right]^2 \right\}
\frac{1}{\sqrt{\pi}} e^{-x^2} dx,
\end{align*}
<!-- -->
by the transformation $y = z - \sqrt{2} \lambda x$, so that
<!-- -->
\begin{align*}
q(x) = \ind\left( z > \sqrt{2} \lambda x \right)
\cdot \frac{1}{z - \sqrt{2} \lambda x}
\exp\left\{ -\frac{1}{2\sigma^2} \left[\log\left( x - \sqrt{2} \lambda x \right) - \mu \right]^2 \right\}
\frac{1}{\sqrt{\pi}}.
\end{align*}
<!-- -->
The function `d_target` defined in `examples/ln-norm/functions.R` computes the target density using this method. 

All codes for this example are in the folder `examples/ln-norm`.

## Constant Majorizer with Numerical Optimization {#sec-ln-norm-const-default}

We may adapt the code from @sec-vmf-const-default to target density in the present problem. Now the weight function may be coded as follows.

```{cpp}
const vws::weight_dfd& w = [&](double x, bool log = true) {
	double out = R_NegInf;
	if (x > 0) {
		out = -std::log(x) - std::pow(std::log(x) - mu, 2) / (2 * std::pow(sigma, 2));
	}
	return log ? out : std::exp(out);
};
```

The base distribution's density, CDF, and quantile function are coded as lambdas and packaged into a `UnivariateHelper` object. Here we can make use of the implementations of the normal distribution in R's API.

```{cpp}
fntl::density df = [&](double x, bool log = false) {
	return R::dnorm(x, z, lambda, log);
};
fntl::cdf pf = [&](double q, bool lower = true, bool log = false) {
	return R::pnorm(q, z, lambda, lower, log);
};
fntl::quantile qf = [&](double p, bool lower = true, bool log = false) {
	return R::qnorm(p, z, lambda, lower, log);
};

vws::UnivariateHelper helper(df, pf, qf);
```

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/ln-norm/ln-norm-v1.cpp")
out1 = r_ln_norm_v1(n = 1000, z, mu, sigma, lambda, N = 50, tol = 0.10)
head(out1$draws)
```

@fig-ln-norm-const-refine plots the bound for the rejection probability during the refinement process, which is captured in the variable `out$lbdd`.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Rate of refine improvement for lognormal-normal example with constant
#|   majorizer.
#| label: fig-ln-norm-const-refine
#| echo: false

data.frame(bdd = exp(out1$lbdd)) %>%
	mutate(step = row_number() - 1) %>%
	ggplot() +
	geom_line(aes(x = step, y = bdd)) +
	scale_y_continuous(n.breaks = 10) +
	xlab("Step") +
	ylab("Bound") +
	theme_minimal()
```

@fig-ln-norm-const-draws plots the empirical distribution of the draws and compares
them to the density.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Empirical distribution of draws versus target for lognormal-normal example
#|   with constant majorizer. The target density is displayed for reference as
#|   the dashed curve.
#| label: fig-ln-norm-const-draws
#| echo: false

gg = data.frame(x = out1$draws) %>%
	ggplot() +
	geom_density(aes(x = x), col = "black") +
	geom_function(fun = d_target, lty = 2,
		args = list(mu = mu, sigma = sigma, z = z, lambda = lambda)) +
	ylab("Empirical Density") +
	theme_minimal()
print(gg)
```

Add an interval based on the $0.025$ and $0.975$ quantiles of the distribution $[Y \mid Z = z]$ approximated from the empirical quantiles of the draws. The value of the observed $z$ and the latent $y$ are also highlighted for reference.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Empirical distribution of draws versus target for Lognormal-Normal example
#|   with constant majorizer. Observed value of $z$ (blue line) and latent value
#|   of $y$ (red line) are displayed along with a 95\% interval (blue ribbon)
#|   based on draws from $[Y \mid Z = z]$.
#| label: fig-ln-norm-interval

interval_lo = quantile(out1$draws, probs = 0.025)
interval_hi = quantile(out1$draws, probs = 0.975)
gg + annotate("rect", xmin = interval_lo, xmax = interval_hi, ymin = 0,
		ymax = Inf, alpha = 0.1, fill = "blue") +
	geom_vline(xintercept = z, col = "blue", lwd = 1.05) +
	geom_vline(xintercept = y_true, col = "red", lwd = 1.05)
```

## Constant Majorizer with Custom Optimization {#sec-ln-norm-const-custom}

The log-weight function $\zeta$ from \eqref{eqn:ln-norm-log-weight-fn} can be both maximized and minimized in closed form. Coding it explicitly reduces computational overhead and avoids convergence issues from numerical optimization. This section will demonstrate how to override the default `optimize` method of `RealConstRegion`.

We have first derivative
<!-- -->
\begin{align*}
&\zeta'(y) = -\frac{1}{y}\left(
1 + \frac{\log y - \mu}{\sigma^2}
\right),
\end{align*}
<!-- -->
for $y \in (0, \infty)$. Let $y^* = \exp(\mu - \sigma^2)$; it is seen that $\zeta'(y)$ is positive when $y < y^*$, negative when $y > y^*$, and takes value zero at $y = y^*$. Therefore, $\zeta$ is unimodal and $y^*$ maximizes $\zeta$ with 
<!-- -->
\begin{align*}
\zeta(y^*)
&= -\log[\exp(\mu - \sigma^2)]  - \frac{(\log [\exp(\mu - \sigma^2)] - \mu)^2}{2 \sigma^2} \\
&= -\mu + \sigma^2 - \frac{(\sigma^2)^2}{2 \sigma^2} \\
&= -\mu + \sigma^2 / 2.
\end{align*}
<!-- -->
Therefore, on a region $\mathcal{D}_j = (\alpha_{j-1}, \alpha_j]$ where both endpoints are smaller than $y^*$, the maximum of $\log w(y)$ is $\log w(\alpha_j)$ and the minimum is $\log w(\alpha_{j-1})$. On the other hand, for a region where both endpoints are larger than $y^*$, the maximum of $\log w(y)$ is $\log w(\alpha_{j-1})$ and the minimum is $\log w(\alpha_j)$.

Here is a plot of $\log w(y)$ with our selected $\mu$ and $\sigma^2$ values.

```{r}
y_star = exp(mu - sigma^2)
w_star = -mu + sigma^2 / 2
printf("Maximizer y = %g obtains value log w(%g) = %g.\n", y_star, y_star, w_star)
```

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Weight function for Lognormal-Normal example on the log-scale, with the
#|   maximizer $y^* = \exp(\mu - \sigma^2)$ highlighted.
#| label: fig-ln-norm-weight
#| echo: false

xlim = y_star + c(-10,10)
w_one = function(y, log = T) {
	out = -Inf
	if (y > 0) { out = -log(y) - (log(y) - mu)^2 / (2*sigma^2) }
	if (log) { return(out) } else { return(exp(out)) }
}
w = Vectorize(w_one, vectorize.args = "y")
ggplot() +
	geom_function(fun = w, xlim = xlim) +
	geom_vline(xintercept = y_star, lty = 2) +
	geom_hline(yintercept = w_star, lty = 2) +
	xlab("y") +
	ylab(expression("log w(y)")) +
	theme_minimal()
```

The maximizer may be coded as follows.

```{cpp}
const vws::optimizer& maxopt = [&](const vws::weight_dfd& w, double lo,
	double hi, bool log)
{
	double y_star = exp(mu - sigma2);
	double out;

	if (y_star > hi) {
		out = w(hi, true);
	} else if (y_star < lo) {
		out = w(lo, true);
	} else {
		out = w(y_star, true);
	}

	return log ? out : exp(out);
};
```

Here is code for the minimizer.

```{cpp}
const vws::optimizer& minopt = [&](const vws::weight_dfd& w, double lo,
	double hi, bool log)
{
	double lwa = w(lo, true);
	double lwb = w(hi, true);
	double out = std::min(lwa, lwb);
	return log ? out : exp(out);
};
```

We can construct the proposal using a single `RealConstRegion` that represents
the support; the `maxopt` and `minopt` arguments are specified here.

```{cpp}
vws::RealConstRegion supp(0, R_PosInf, w, helper, maxopt, minopt);
```

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/ln-norm/ln-norm-v2.cpp")
out2 = r_ln_norm_v2(n = 1000, z, mu, sigma, lambda, N = 50, tol = 0.10)
head(out2$draws)
```

## Linear Majorizer {#sec-ln-norm-linear}

We can obtain a linear majorizer by noting the convexity of $\zeta$. Its second derivative is seen to be
<!-- -->
\begin{align*}
&\zeta''(y) = -\frac{1}{y^2} \left(
1 + \frac{\log y - \mu - 1}{\sigma^2}
\right),
\end{align*}
<!-- -->
which has root $y^0 = \exp\{ \mu - \sigma^2 + 1 \}$. The weight function $w$ is log-concave for $y < y^0$ and log-convex for $y > y^0$. This is plotted in @fig-ln-norm-weight-convexity. We will assume that there are two initial regions $\mathcal{D}_1 = (0, \exp(\mu - \sigma^2 + 1)]$ and $\mathcal{D}_2 = (\exp(\mu - \sigma^2 + 1), \infty]$; this will to ensure that all partitions considered thereafter consist of regions on which $\zeta$ is entirely concave or convex.

```{r}
y0 = exp(mu - sigma^2 + 1)
printf("Convexity changes at y = %g.\n", y0)
```


**TBD: MGF, code snippets**




```{r}
#| out-width: 60%
#| fig-cap: |
#|   Weight function for Lognormal-Normal example on the log-scale,
#|   highlighting $y^0 = \exp(\mu - \sigma^2 + 1)$ where there is a change in
#|   convexity.
#| label: fig-ln-norm-weight-convexity
#| echo: false

xlim = c(0, 2000)
ggplot() +
	geom_function(fun = w, xlim = xlim) +
	geom_vline(xintercept = y0, lty = 2) +
	xlab("y") +
	ylab(expression("log w(y)")) +
	theme_minimal()
```

As in @sec-vmf-linear, we can majorize $w$ with exponentiated linear functions of the form $\overline{w}_j(y) = \exp\{ \overline{\beta}_{j0} + \overline{\beta}_{j1} y \}$. Here, we also assume a minorizer of the form $\underline{w}_j(y) = \exp\{ \underline{\beta}_{j0} + \underline{\beta}_{j1} y \}$. Before proceeding, the following integral is stated as a remark as it will be used several times.

::: {#rem-normal-mgf-integral}
Let $\phi(\cdot \mid \mu, \sigma^2)$ and $\Phi(\cdot \mid \mu, \sigma^2)$ be the density and CDF of $Y \sim \text{N}(\mu, \sigma^2)$, respectively. If $a < b$ are scalars (possibly infinite), then 
<!-- -->
\begin{align*}
\int_a^b e^{ty} \phi(y \mid \mu, \sigma^2) dy
= \exp(\mu t + t^2 \sigma^2 / 2) \left\{ 
\Phi(b \mid \mu + t \sigma^2, \sigma^2) - \Phi(a \mid \mu + t \sigma^2, \sigma^2)
\right\}.
\end{align*}
<!-- -->
The special case $a = -\infty$ and $b = \infty$ yields the moment-generating function $M(t) = \exp(\mu t + t^2 \sigma^2 / 2)$ of $Y$.
:::

@rem-normal-mgf-integral yields the expression
<!-- -->
\begin{align*}
\overline{\xi}_j
&= \int_{\alpha_{j-1}}^{\alpha_j} \overline{w}_j(y) g(y) dy \\
%
&= \exp\{ \overline{\beta}_{j0} \} \int_{\alpha_{j-1}}^{\alpha_j} 
\exp\{ \overline{\beta}_{j1} y \}
\frac{1}{\lambda \sqrt{2\pi}}
\exp\left\{ -\frac{1}{2 \lambda^2} (y - z)^2 \right\} dy \\
%
&= \exp\left\{
\overline{\beta}_{0j} + z \overline{\beta}_{1j}
+ \frac{1}{2} \overline{\beta}_{1j}^2 \lambda^2
\right\}
\left\{
\Phi(\alpha_j \mid z + \overline{\beta}_{1j} \lambda^2, \lambda^2)
- \Phi(\alpha_{j-1} \mid z + \overline{\beta}_{1j} \lambda^2, \lambda^2)
\right\},
%\label{eqn:ln-norm-xi-upper}
\end{align*}
<!-- -->
Similarly,
<!-- -->
\begin{align*}
\underline{\xi}_j
&= \int_{\alpha_{j-1}}^{\alpha_j} \underline{w}_j(y) g(y) dy \\
&= \exp\left\{
\underline{\beta}_{0j} + z \underline{\beta}_{1j}
+ \frac{1}{2} \underline{\beta}_{1j}^2 \lambda^2
\right\}
\left\{
\Phi(\alpha_j \mid z + \underline{\beta}_{1j} \lambda^2, \lambda^2)
- \Phi(\alpha_{j-1} \mid z + \underline{\beta}_{1j} \lambda^2, \lambda^2)
\right\}.
%\label{eqn:ln-norm-xi-lower}
\end{align*}
<!-- -->
The proposal $h$ is a finite mixture $h(y) = \sum_{j=1}^N \pi_j g_j(y)$ with
<!-- -->
\begin{align*}
g_j(y)
&= \overline{w}_j(y) g(y) \ind\{ y \in \mathcal{D}_j \} / \overline{\xi}_j \\
%
&= \exp\{ \overline{\beta}_{j0} + \overline{\beta}_{j1} y \}
\frac{1}{\lambda \sqrt{2\pi}}
\exp\left\{ -\frac{1}{2 \lambda^2} (y - z)^2 \right\}
\cdot \ind(\alpha_{j-1} < y \leq \alpha_j) / \overline{\xi}_j  \\
%
&= \frac{
\frac{1}{\lambda \sqrt{2\pi}}
\exp\left\{ -\frac{1}{2 \lambda^2} (y - \lambda^2 \overline{\beta}_{1j})^2 \right\}
}{
\Phi(\alpha_j \mid z + \lambda^2 \overline{\beta}_{1j}, \lambda^2)
- \Phi(\alpha_{j-1} \mid z + \lambda^2 \overline{\beta}_{1j}, \lambda^2)
}
\cdot \ind(\alpha_{j-1} < y \leq \alpha_j),
\end{align*}
<!-- -->
which is the density of $\text{N}(z + \lambda^2 \overline{\beta}_{1j}, \lambda^2)$ truncated to the interval $(\alpha_{j-1} ,\alpha_j]$.

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/ln-norm/ln-norm-v3.cpp")
out3 = r_ln_norm_v3(n = 1000, z, mu, sigma, lambda, lo = 1e-8, 1e8,
	N = 50, tol = 0.10)
head(out3$draws)
```

@fig-ln-norm-linear-draws plots the empirical distribution of the draws and compares
them to the density.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Empirical distribution of draws versus target for lognormal-normal example
#|   with constant majorizer. The target density is displayed for reference as
#|   the dashed curve.
#| label: fig-ln-norm-linear-draws
#| echo: false

data.frame(x = out1$draws) %>%
	ggplot() +
	geom_density(aes(x = x), col = "black") +
	geom_function(fun = d_target, lty = 2,
		args = list(mu = mu, sigma = sigma, z = z, lambda = lambda)) +
	ylab("Empirical Density") +
	theme_minimal()
```

@fig-ln-norm-refine plots the bound for the rejection probability for this sampler after each step of refining, along with those from the previous two versions. A substantial improvement in efficiency is seem here; fewer regions are needed to achieve a small rejection probability. Although versions 1 and 2 of the sampler are based on the same proposal, the changes to their bounds are seen to differ due to randomness in knot selection.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Rate of refine improvement for lognormal-normal example with constant
#|   majorizer.
#| label: fig-ln-norm-refine
#| echo: false

df1 = data.frame(sampler = "v1", bdd = exp(out1$lbdd)) %>% 
	mutate(N = row_number())
df2 = data.frame(sampler = "v2", bdd = exp(out2$lbdd)) %>% 
	mutate(N = row_number())
df3 = data.frame(sampler = "v3", bdd = exp(out3$lbdd)) %>% 
	mutate(N = row_number() + 1)
df = bind_rows(df1, df2, df3) %>% mutate(N = as.integer(N))

ggplot(df) +
	geom_line(aes(x = N, y = bdd, color = sampler), lwd = 1.02, alpha = 1) +
	geom_point(aes(x = N, y = bdd, pch = sampler, color = sampler),
		cex = 3, alpha = 1) +
	scale_x_continuous(breaks = seq(0, max(df$N))) +
	scale_y_continuous(n.breaks = 10) +
	xlab("N") +
	ylab("Bound") +
	theme_minimal()
```















(**TBD: LEFT OFF HERE! ALSO NEED TO CODE THE SAMPLER!**)
To construct $\overline{\beta}_{j0}$ and $\overline{\beta}_{j1}$, we make use of method using the expression
<!-- -->
\begin{align*}
M_j(s) = \int_{\alpha_{j-1}}^{\alpha_j} e^{sx} g(x) dx
= \frac{e^{s\alpha_j} - e^{s\alpha_{j-1}}}{s(\alpha_j - \alpha_{j-1})}.
\end{align*}
<!-- -->
To compute \eqref{eqn:bound}, we assume the "trivial" minorizer $\underline{w}_j(x) = w(x)$ so that
<!-- -->
\begin{align*}
\underline{\xi}_j
= \int_{\alpha_{j-1}}^{\alpha_j} w(x) g(x) dx
= \int_{\alpha_{j-1}}^{\alpha_j} \frac{(1-x^2)^{(d-3)/2} \kappa e^{\kappa x}}{e^{\kappa \alpha_N} - e^{\kappa \alpha_0}} dx.
\end{align*}

The proposal is implemented with the `vws` package by inheriting from the abstract `Region` base class (@sec-cpp-api-region) and implementing each of the functions using the expressions above. We name this subclass `LinearVWSRegion`; its complete code is given in `examples/vmf/LinearVWSRegion.h`. The code in `examples/vmf/vmf-v3.cpp` instantiates a proposal with regions of type `LinearVWSRegion`, invokes rejection sampling, and returns an `Rcpp::List` with the results. This code is displayed below.

```{.cpp include="examples/ln-norm/ln-norm-v3.cpp" code-line-numbers="true"}
```

The following R code builds the complete example and invokes the sampling function.

```{r}
Rcpp::sourceCpp("examples/ln-norm/ln-norm-v3.cpp")
out = r_ln_norm_v3(n = 1000, kappa = 5, d = 4, N = 50, tol = 0.10)
head(out$draws)
```


::: {.callout-caution title="TBD"}
Everything after this point is old material ...

We need to pick finite endpoints I think ... 
:::





The following construction assumes that $\zeta$ is finite and concave on the interval $\mathcal{D}_j = (\alpha_{j-1}, \alpha_j]$. A majorizer is obtained from
<!-- -->
\begin{align}
\zeta(y) &\leq \zeta(c) + (y - c) \nabla(c)
= \overline{\beta}_{j0} + \overline{\beta}_{j1} y,
\quad 
\overline{\beta}_{j0} = \zeta(c) - c \cdot \zeta'(c), \quad
\overline{\beta}_{j1} = \zeta'(c),
\label{eqn:ln-norm-linear-majorizer}
\end{align}
<!-- -->
where $c$ is a point in $\mathcal{D}_j$. In particular, let us consider the value of $c$ as
<!-- -->
\begin{align*}
c^* &= \argmin_{c \in \mathcal{D}_j} \int_{\mathcal{D}_j} | h_0(y) - f_0(y) | dy
\nonumber \\
%
&\equiv \argmin_{c \in \mathcal{D}_j} \Big\{ \log w(c) - c \nabla(c) + \log M_j(\nabla(c)) \Big\}.
\end{align*}
<!-- -->
Here, $M_j(s)$ is the moment generating function of random variable $T \sim g$ with support truncated to $(\alpha_{j-1}, \alpha_j]$:
<!-- -->
\begin{align*}
M_j(s)
%
&= \int_{\alpha_{j-1}}^{\alpha^j} e^{sy} \frac{
\phi(y \mid z, \lambda^2)
}{
\Phi(\alpha_j \mid z, \lambda^2) - \Phi(\alpha_{j-1} \mid z, \lambda^2)
} dy \\
%
&= \exp(z s + s^2 \lambda^2 / 2) 
\frac{
\Phi(\alpha_j \mid z + s \lambda^2, \lambda^2) - \Phi(\alpha_{j-1} \mid z + s \lambda^2, \lambda^2)
}{
\Phi(\alpha_j \mid z, \lambda^2) - \Phi(\alpha_{j-1} \mid z, \lambda^2)
},
\end{align*}
<!-- -->
using [@rem-normal-mgf-kernel]. A minorizer can be obtained by expressing $y = (1 - \lambda) \alpha_{j-1} + \lambda \alpha_j$ for $\lambda \in [0,1]$ so that $\lambda = (y - \alpha_{j-1}) / (\alpha_j - \alpha_{j-1})$ and
<!-- -->
\begin{align}
\zeta(y) &\geq (1-\lambda) \zeta(\alpha_{j-1}) + \lambda \zeta(\alpha_j) \nonumber \\
&= \zeta(\alpha_{j-1}) + \frac{y - \alpha_{j-1}}{\alpha_j - \alpha_{j-1}} [ \zeta(\alpha_j) - \zeta(\alpha_{j-1})]
= \underline{\beta}_{j0} + \underline{\beta}_{j1} y,
\label{eqn:ln-norm-linear-minorizer}
\end{align}
<!-- -->
with
<!-- -->
\begin{align*}
\underline{\beta}_{j0} = \zeta(\alpha_{j-1}) - \alpha_{j-1} \underline{\beta}_{j1}
\quad \text{and} \quad
\underline{\beta}_{j1} = \frac{\zeta(\alpha_j) - \zeta(\alpha_{j-1}) }{ \alpha_j - \alpha_{j-1} }.
\end{align*}
<!-- -->
If $\zeta$ is convex on $\mathcal{D}_j$ rather than concave, the roles of \eqref{eqn:ln-norm-linear-majorizer} and \eqref{eqn:ln-norm-linear-minorizer} are switched so that \eqref{eqn:ln-norm-linear-majorizer} is minorizer and \eqref{eqn:ln-norm-linear-minorizer} is majorizer.

# Example: Bessel Count Distribution {#sec-bessel}

::: {.callout-caution title="TBD"}
Under construction. The example is working, but the writing needs to be fleshed out.
:::

TBD: this sampling problem comes up in @Devroye2002.

```{r}
#| prompt: true
source("examples/bessel/bessel.R")
```

In this example, we consider generating from the Bessel density described by @Devroye2002 as
<!-- -->
\begin{align*}
f(x) = \frac{(a/2)^{2x+\nu}}{I_v(a) \cdot x! \cdot \Gamma(x + \nu + 1)}
\cdot \ind\{ x \in \mathbb{N} \}, \quad
\nu > -1, \quad
a > 0,
\end{align*}
<!-- -->
where $\mathbb{N} = \{ 0, 1, 2, \ldots \}$ is the set of nonnegative integers
and
<!-- -->
\begin{align*}
I_v(a) = \sum_{x=0}^\infty \frac{(a/2)^{2x+\nu}}{x! \cdot \Gamma(x + \nu + 1)}
\end{align*}
<!-- -->
is a modified Bessel function of the first kind.

At the time the paper was written, @Devroye2002 considered it difficult to
generate exact draws from this distribution. He develops what appears to be a customized accept-reject method after establishing properties of the distribution. We can draw from this distribution fairly easily without such an in-depth analysis.

Our approach is to decompose the density into
<!-- -->
\begin{align*}
f(x)
&= \frac{(a/2)^{2x+\nu}}{I_v(a) \cdot x! \cdot \Gamma(x + \nu + 1)}
\cdot \ind\{ x \in \mathbb{N} \} \\
%
&\propto 
\underbrace{\frac{1}{\Gamma(x + \nu + 1)}}_{w(x)}
\underbrace{\frac{(a/2)^{2x} e^{-a^2/4}}{x!} \ind\{ x \in \mathbb{N} \}}_{g(x)},
\end{align*}
<!-- -->
so that $g$ is the density of a $\text{Poisson}(a^2/4)$ distribution and the weight function is specified by $\log w(x) = -\log \Gamma(x + \nu + 1)$. Notice that we have disregarded many of the normalizing constants in this formulation.

We can explicitly describe computations involved in the sampler.

- Make regions of the form $\mathcal{D}_j = (\alpha_{j-1}, \alpha_j]$.
- Compute $P(T \in \mathcal{D}_j)$.
- Optimize $w(x)$ on $\mathcal{D}_j$ to obtain $\overline{w}_j$ and
	  $\underline{w}_j$.
- Draw from Poisson truncated to $\mathcal{D}_j$.
- Refine algorithm that uses integer midpoint.

We can also show some displays of how the sampler performs.

- Volume and/or log volume as we increase $N$ with refining.
- The exact density versus the proposal density for several values of $N$ along the refinement path.
- The number of draws rejected in practice along the refinement path (say, if we request a large number such as $n = 10^7$).

It looks like we can capture the distribution almost exactly by covering the entire support with a not-too-large choice of $N$, but this will depend on the parameters $a$ and $\nu$. When we have refined to this point, it becomes extremely rare to reject any candidates.

Consider giving background information about the Bessel distribution to show why it was of special interest to Devroye.

```{r}
#| out-width: 60%
#| fig-cap: |
#|   Rate of refine improvement on the log-scale for Bessel example.
#| label: fig-bessel-refine

nu = -0.5
lambda = 10

helper = poisson_helper(lambda^2 / 4)

w = function(x, log = TRUE) {
	out = -lgamma(x + nu + 1) + lambda^2 / 4 + nu * log(lambda / 2)
	if (log) { return(out) } else { return(exp(out)) }
}

support = IntConstRegion$new(a = -1, b = Inf, w = w, g = helper)
regions = list(support)

h_init = FMMProposal$new(regions)
refine_out = refine(h_init, N = 30, report = 10)
h = refine_out$h

data.frame(logbdd = refine_out$log_bdd_hist) %>%
	mutate(step = row_number() - 1) %>%
	ggplot() +
	geom_line(aes(x = step, y = logbdd)) +
	scale_y_continuous(n.breaks = 10) +
	xlab("Step") +
	ylab("Log of Bound") +
	theme_minimal()

ctrl = rejection_control(max_rejects = 10000, report = 5000)
out = rejection(h, n = 10000, control = ctrl)
x = unlist(out)
```

Plot the empirical distribution of the draws with the target density.

```{r}
#| out-width: 60%
#| fig-cap: Empirical distribution of draws versus target for Bessel example.
#| label: fig-bessel-draws

n = length(x)
tab_out = factor(x, levels = seq(0, 11)) |> table()
x_vals = tab_out |> names() |> as.integer()
d_vals_emp = (tab_out / n) |> as.numeric()
d_vals = d_bessel(x_vals, a = lambda, nu = nu)

data.frame(x = x_vals, d_emp = d_vals_emp, d = d_vals) %>%
	ggplot() +
	geom_point(aes(x = x, y = d), pch = 1) +
	geom_point(aes(x = x, y = d_emp), pch = 3) +
	ylab("Probability") +
	theme_minimal()
```

Plot proposal versus the target on the log-scale.

```{r}
#| out-width: 60%
#| fig-cap: Proposal versus target on the log-scale for Bessel example.
#| label: fig-bessel-proposal

log_h = function(x) { sapply(x, h$d, log = TRUE) }
log_f = function(x) { d_bessel(x, a = lambda, nu = nu, log = TRUE) }

df = data.frame(x = x_vals) %>%
	mutate(log_h_vals = log_h(x)) %>%
	mutate(log_f_vals = log_f(x))

ggplot(df) +
	geom_point(aes(x, log_h_vals), pch = 1) +
	geom_point(aes(x, log_f_vals), pch = 3) +
	xlab("x") +
	ylab("Density") +
	theme_minimal()
```


## Constant Majorizer with Numerical Optimization {#sec-bessel-const-default}

```{.cpp include="examples/bessel/bessel-v1.cpp" code-line-numbers="true"}
```

## Constant Majorizer with Custom Optimization {#sec-bessel-const-custom}

**TBD:** Write stuff

```{.cpp include="examples/bessel/bessel-v2.cpp" code-line-numbers="true"}
```

## Linear Majorizer {#sec-bessel-linear}

**TBD:** Is it tractable for this problem?


# Conclusions

::: {.callout-caution title="TBD"}
Content needed
:::

# Acknowledgments {.unlisted}

::: {.callout-caution title="TBD"}
Content needed
:::

# References
